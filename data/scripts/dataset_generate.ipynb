{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae6338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b131b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model input\n",
    "prompt = \"\"\"# your role\n",
    "你是一个一对一教英语自然拼读课的老师\n",
    "Your name is Lucy.\n",
    "You are a twenty six year-old English teacher from Seattle, USA.\n",
    "\n",
    "# teaching objective\n",
    "- 目标单词：kate\n",
    "- 目标单词音标列表：[/k/, /ei/, /t/]\n",
    "\n",
    "# dialogue history\n",
    "\n",
    "\n",
    "# student input\n",
    "输入描述：拼读错误：一个错误\n",
    "错误音素：[/ei/]\n",
    "\n",
    "# 特殊音标标记\n",
    "/l_1/:/l/后跟元音时， 使用/l_1/，如单词left、led、love、like、alligator。\n",
    "/l_2/:/l/后跟辅音或在单词结尾，使用/l_2/，如单词ball、feel、help。\n",
    "/r_1/:/r/后跟元音时，使用/r_1/，示例单词：red、run、road、break。\n",
    "\n",
    "# response strategy\n",
    "- part1:纠错+讲解\n",
    "    - 迁移示例:\n",
    "        - 目标单词:cake,错误音素:/eɪ/,拼读规则:[/k/, /eɪ/, /k/],回复示例:Remember A, space, E, says /eɪ/.\n",
    "        - 目标单词:car,错误音素:/ɑːr/,拼读规则:[/k/, /ɑːr/],回复示例:Remember A, R, makes the /ɑːr/ sound.\n",
    "        - 目标单词:red,错误音素:/e/,拼读规则:[/r_1/, /ed/],回复示例:Take note of letter E. It says /e/.\n",
    "        - 目标单词:red,错误音素:/d/,拼读规则:[/r_1/, /ed/],回复示例:Take note of letter D. It says /d/.\n",
    "        - 目标单词:mat,错误音素:/t/,拼读规则:[/m/, /æ/, /t/],回复示例:T says /t/.\n",
    "        - 目标单词:red,错误音素:/r/,拼读规则:[/r_1/, /ed/],回复示例:Pay attention, R, says /r_1/.\n",
    "        - 目标单词:mat,错误音素:/t/,拼读规则:[/m/, /æ/, /t/],回复示例:Keep in mind that T makes the /t/ sound.\n",
    "        - 目标单词:mat,错误音素:/m/,拼读规则:[/m/, /æ/, /t/],回复示例:Please remember M says /m/.\n",
    "        - 目标单词:mat,错误音素:/æ/,拼读规则:[/m/, /æ/, /t/],回复示例:Remember A says /æ/. \n",
    "        - 目标单词:cake,错误音素:/eɪ/,拼读规则:[/k/, /eɪk/],回复示例:Pay attention, A, space, E says /eɪ/.\n",
    "        - 目标单词:car,错误音素:/ɑːr/,拼读规则:[/k/, /ɑːr/],回复示例:Keep in mind that A, R, makes the /ɑːr/ sound.\n",
    "- part2:重读目标单词\n",
    "    - 迁移示例:\n",
    "        - 目标单词:car,错误音素:/k/,拼读规则:[/k/, /ɑːr/],回复示例:Say, /k/, /k/, car.\n",
    "        - 目标单词:shirt,错误音素:/ɜːr/,拼读规则:[/ʃ/, /ɜːr/, /t/],回复示例:Please say, /ɜːr/, /ɜːr/, shirt.\n",
    "        - 目标单词:mat,错误音素:/æ/,拼读规则:[/m/, /æ/, /t/],回复示例:Please say, /æ/, /æ/, mat.\n",
    "        - 目标单词:shirt,错误音素:/ʃ/,拼读规则:[/ʃ/, /ɜːr/, /t/],回复示例:Please say, /ʃ/, /ʃ/, shirt.\n",
    "        - 目标单词:red,错误音素:/r/,拼读规则:[/r_1/, /e/, /d/],回复示例:Please follow me, /r_1/, /r_1/, red.\n",
    "        - 目标单词:mat,错误音素:/t/,拼读规则:[/m/, /æ/, /t/],回复示例:Repeat after me, /t/, /t/, mat.\n",
    "        - 目标单词:mat,错误音素:/æ/,拼读规则:[/m/, /æt/],回复示例:Please say, /æt/, /æt/, mat.\n",
    "        - 目标单词:mat,错误音素:/m/,拼读规则:[/m/, /æ/, /t/],回复示例:Follow me, /m/, /m/, mat.\n",
    "        - 目标单词:red,错误音素:/d/,拼读规则:[/r_1/, /ed/],回复示例:Follow me, /ed/, /ed/, red.\n",
    "        - 目标单词:cake,错误音素:/eɪ/,拼读规则:[/k/, /eɪk/],回复示例:Please say, /eɪk/, /eɪk/, cake.\n",
    "        - 目标单词:red,错误音素:/d/,拼读规则:[/r_1/, /e/, /d/],回复示例:Follow me, /d/, /d/, red.\n",
    "        - 目标单词:red,错误音素:/e/,拼读规则:[/r_1/, /e/, /d/],回复示例:Please follow me, /e/, /e/, red.\n",
    "        - 目标单词:mat,错误音素:/t/,拼读规则:[/m/, /æ/, /t/],回复示例:Let's try once more, /t/, /t/, mat.\n",
    "        - 目标单词:red,错误音素:/e/,拼读规则:[/r_1/, /ed/],回复示例:Follow me, /ed/, /ed/, red.\n",
    "        - 目标单词:car,错误音素:/ɑːr/,拼读规则:[/k/, /ɑːr/],回复示例:Say, /ɑːr/, /ɑːr/, car.\n",
    "        - 目标单词:mat,错误音素:/æ/,拼读规则:[/m/, /æt/],回复示例:Now say, /æt/, /æt/, mat.\n",
    "        - 目标单词:cake,错误音素:/eɪ/,拼读规则:[/k/, /eɪ/, /k/],回复示例:Say, /eɪ/, /eɪ/, cake.\n",
    "        - 目标单词:mat,错误音素:/t/,拼读规则:[/m/, /æt/],回复示例:Please say, /æt/, /æt/, mat.\n",
    "\n",
    "\n",
    "# task\n",
    "- **任务1**：输出目标单词的拼读规则\n",
    "    - sub_step1: 判断<目标单词音标列表>是否是乱序的，如果是乱序的则需要调整顺序，**只能调整列表中元素的顺序，不能合并元素**\n",
    "    - sub_step2: 如果<目标单词音标列表>中包含/l/或/r/，根据<特殊音标标记>的规则修改音标\n",
    "    - sub_step3: 输出调整后的音标列表作为拼读规则\n",
    "    - example\n",
    "        - 目标单词：mane, 目标单词音标列表: [/n/, /eɪ/, /m/], 调整后: [/m/, /eɪ/, /n/]\n",
    "        - 目标单词：mat, 目标单词音标列表: [/æ/, /t/, /m/], 调整后: [/m/, /æ/, /t/]\n",
    "        - 目标单词：red, 目标单词音标列表: [/d/, /r/, /e/], 调整后: [/r_1/, /e/, /d/]\n",
    "- **任务2**：遍历<response strategy>中每个part，把每个part生成的回复按顺序拼接到一起，作为最终的回复\n",
    "    - 如果part中有给出通用示例的的，直接从通用示例选择一个，只使用一个示例\n",
    "    - 如果part中有给出迁移示例的（1条迁移示例包含：目标单词、错误音素（可选）、拼读规则、回复示例），结合任务1生成的拼读规则，以及<teaching objective>中目标单词，<student input>中的错误音素（如有），从迁移示例中匹配一个最佳示例，**匹配时要优先考虑能够产生错误音素的单词拼写规则或字母组合（例如，单词shame中的/eɪ/音是由'a_e'拼写规则产生的，因此应匹配教授'A, space, E'的示例）**，只基于回复示例生成回复，不要添加多余的描述，只使用一个示例\n",
    "    - 严格遵守任务1输出的拼读规则，不要修改拼读规则\n",
    "    - 使用空格拼接多个part的回复\n",
    "\n",
    "# requirement\n",
    "- 最终的回复不能有换行符\n",
    "- 最终的回复必须严格完全由各个part构成，确保没有其他内容\n",
    "\n",
    "# output\n",
    "- 输出且只输出任务2的回复（输出内容中不要有换行符、双引号、单引号。不要遗漏逗号、句号。）\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06523f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e515708a99da4bb48b81ec4ffd82cba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "model_name = \"/root/group-shared/models/base_models/Qwen3-32B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5023286",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aba419ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea546a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1/16 [00:06<01:43,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: Please remember K says /k/\\. Pay attention, A says /ei/\\. T says /t/\\.\\.\\. Please say, /k/, /k/, kate\\. Please say, /ei/, /ei/, kate\\. Please say, /t/, /t/, kate\\.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 2/16 [00:14<01:39,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: Remember K says /k/. Take note of letter A. It says /ei/. Keep in mind that T makes the /t/ sound. Please say, /k/, /k/, kate. Please say, /ei/, /ei/, kate. Please say, /t/, /t/, kate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3/16 [00:19<01:19,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: Remember K says /k/. Take note of letter A. It says /ei/. Keep in mind that T makes the /t/ sound. Please say, /k/, /k/, kate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 4/16 [00:26<01:18,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: Remember K says /k/. Take note of letter A. It says /ei/. Keep in mind that T makes the /t/ sound. Please say, /k/, /k/, kate. Please say, /ei/, /ei/, kate. Please say, /t/, /t/, kate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 5/16 [00:29<01:00,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: Remember K says /k/, E says /ei/, T says /t/. Please say, /k/, /k/, kate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 6/16 [00:33<00:48,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: Remember K says /k/, E says /ei/, T says /t/. Please say, /k/, /k/, kate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 7/16 [00:40<00:50,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: Remember K says /k/. Take note of letter A. It says /ei/. Keep in mind that T makes the /t/ sound. Please say, /k/, /k/, kate. Please say, /ei/, /ei/, kate. Please say, /t/, /t/, kate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [00:47<00:49,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: Remember K says /k/. Take note of letter A. It says /ei/. Keep in mind that T makes the /t/ sound. Please say, /k/, /k/, kate. Please say, /ei/, /ei/, kate. Please say, /t/, /t/, kate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 9/16 [00:54<00:44,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: Remember K says /k/. Take note of letter A. It says /ei/. Please remember T says /t/. Please say, /k/, /k/, kate. Please say, /ei/, /ei/, kate. Please say, /t/, /t/, kate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 10/16 [00:59<00:35,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: Remember K, says /k/. Take note of letter A. It says /ei/. Keep in mind that T makes the /t/ sound. Please say, /k/, /k/, kate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 11/16 [01:04<00:27,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: Please remember K says /k/\\. Pay attention, A says /ei/\\. Keep in mind that T makes the /t/ sound\\. Please say, /k/, /k/, kate\\.\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(16)):\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5530d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "# 1. 先将模型移到CPU（可选，但推荐）\n",
    "model = model.cpu()\n",
    "\n",
    "# 2. 删除模型变量\n",
    "del model\n",
    "\n",
    "# 3. 强制垃圾回收\n",
    "gc.collect()\n",
    "\n",
    "# 4. 清空CUDA缓存\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d89a663b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [59:34<00:00,  3.49s/it] \n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "# conduct text completion\n",
    "for _ in tqdm(range(1024)):\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768,\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    data = {}\n",
    "    data[\"prompt\"] = prompt\n",
    "    data[\"response\"] = content\n",
    "    data[\"reward\"] = (\"K says /k/, E says /ei/, T says /t/\" in content) * 1.0 + (\"K, A, T makes the /k/, /ei/, /t/ sound\" in content) * 1.0 + (\"/k/, /ei/, /t/, Kate\" in content) * 1.0 - 1.0\n",
    "    # print(data[\"response\"], data[\"reward\"])\n",
    "    dataset.append(data)\n",
    "\n",
    "with open(\"../train_1024_prompt.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15019cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22891388adec4fceab8a7f01f4cbce7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'response', 'reward'],\n",
      "    num_rows: 1024\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'reward'],\n",
      "        num_rows: 1024\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39e62b4a2f445a49f0070a51617ce83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"../train_1024_prompt.json\",split=\"train\")\n",
    "print(dataset)\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": dataset\n",
    "})\n",
    "print(dataset_dict)\n",
    "if os.path.exists(\"../aixue_train_1024_prompt\"):\n",
    "    shutil.rmtree(\"../aixue_train_1024_prompt\")\n",
    "    os.makedirs(\"../aixue_train_1024_prompt\", exist_ok=True)\n",
    "dataset_dict.save_to_disk(\n",
    "    dataset_dict_path=\"../aixue_train_1024_prompt\",\n",
    "    max_shard_size=\"500MB\",  # 可选：分片大小控制\n",
    "    num_proc=1,               # 可选：并行进程数\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ac1af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aixue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
