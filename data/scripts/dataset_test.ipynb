{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe90413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ca3727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a0aeb74bf44c58b8e083763c34c491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'prompt', 'response', 'reward'],\n",
      "    num_rows: 480\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'prompt', 'response', 'reward'],\n",
      "        num_rows: 480\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a273aae24148e5b97dd2f2573a5cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"../train.json\",split=\"train\")\n",
    "print(dataset)\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": dataset\n",
    "})\n",
    "print(dataset_dict)\n",
    "if os.path.exists(\"../aixue_test_data\"):\n",
    "    shutil.rmtree(\"../aixue_test_data\")\n",
    "    os.makedirs(\"../aixue_test_data\", exist_ok=True)\n",
    "dataset_dict.save_to_disk(\n",
    "    dataset_dict_path=\"../aixue_test_data\",\n",
    "    max_shard_size=\"500MB\",  # 可选：分片大小控制\n",
    "    num_proc=1,               # 可选：并行进程数\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f410e",
   "metadata": {},
   "source": [
    "# Response Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c58e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b4a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model input\n",
    "prompt = \"\"\"你是一名专注于1对1自然拼读教学的英语老师。\n",
    "\n",
    "# 教学目标\n",
    "## 核心课程目标：\n",
    "    1. 建立“字母/字母组合”与“发音”的牢固对应关系（音形联结），掌握单个字母基础发音及常见字母组合发音。\n",
    "    2. 发展学生语音意识（音素、音节、重音感知），提升单词记忆与拼写能力。\n",
    "    3. 最终目标：实现“见词能读，听音能写”。\n",
    "## 单节课教学目标：\n",
    "    1. 个性化教学：根据学生能力调整内容呈现和练习方式，激发兴趣，提升效果。\n",
    "    2. 保持专注与信心：控制单次学习时长，避免连续错误过多，减少疲劳感和挫败感。\n",
    "\n",
    "# 学生画像\n",
    "    1. 年龄：7岁\n",
    "    2. 性别：女\n",
    "    3. 所在地：中国三线城市\n",
    "    4. 英语基础：\n",
    "       - 掌握26个英文字母名称。\n",
    "       - 仅会极少量简单会话（如：What's your name?）。\n",
    "\n",
    "# 当前教学状态\n",
    "    1. 课节内容：教授字母 A、B、C 的发音（a: /æ/, b: /b/, c: /k/）。\n",
    "    2. 当前**环节**：字母 A (/æ/) 的发音练习。\n",
    "    3. 主题关联：教学围绕'苹果 (apple)'展开，练习部分如有单词，建议与之相关。\n",
    "\n",
    "# 教学工具箱 (可选学习范式)\n",
    "## 字母教学包含4种基础练习类型（难度递增）, 例如对于字母a：\n",
    "1. 纯音素重复：`/æ/ /æ/ /æ/` (重复发音3次)\n",
    "2. 音形对应：`a says /æ/` (建立字母与发音关联)\n",
    "3. 音素-单词关联：`/æ/ /æ/ apple` (强化发音在单词中的感知)\n",
    "4. 综合练习：`a says /æ/, /æ/ /æ/ apple` (整合字母、发音与单词)\n",
    "## 智能纠错策略 (根据错误类型选择下一步)\n",
    "1. 错误类型A (字母名称错，发音对)：例如学生说'a says /æ/' (a读错，/æ/正确)。  \n",
    "    **下一步：** 聚焦字母名称练习。老师示范：'a' (仅字母名称)。\n",
    "2. 错误类型B (单词发音错，字母发音对)：例如学生说'/æ/ /æ/ apple' (apple发音错，/æ/正确)。  \n",
    "    **下一步：** 聚焦目标单词练习。老师示范：'apple' (仅单词)。\n",
    "## 教学控制参数\n",
    "1. 单字母(单环节)最大教学次数：4次 (若学生能力强，`综合练习`一次性通过，可减少次数, 直接进入下一个**环节**)。\n",
    "2. 单次跟读最大重复次数：1-2次 (避免疲劳)。\n",
    "3. 核心原则：及时强化正确，精准纠正错误，保持学习动力。\n",
    "\n",
    "# 学生学习记录\n",
    "1. 学习次数：1 次\n",
    "2. 上次练习内容：'/æ/ /æ/ apple'\n",
    "3. 上次表现评分：B (部分正确)\n",
    "4. 具体错误：单词'apple'中的辅音'/p/'发音有瑕疵。\n",
    "\n",
    "# 你的任务：制定下一步教学指令\n",
    "1. **评估：** 基于教学目标、学生基础、当前环节、可选范式、纠错策略、历史表现及教学控制参数，决定下一步的教学。\n",
    "2. 输出格式要求：\n",
    "   - 如果结束当前字母教学，直接回复 <END>\n",
    "   - 如果需要继续学习，直接回复 下一步跟读的句子\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83d29acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff14edc9b194dee9bb2bb2b84458d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：`apple`\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**'apple'** (仅单词，聚焦单词发音练习，纠正辅音 /p/)\n",
      "thinking content: \n",
      "content: **下一步跟读的句子：**  \n",
      "`apple` (仅单词练习，强化辅音 /p/ 发音)\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：\n",
      "\n",
      "**\"apple\"** (仅单词，针对错误类型B，聚焦单词发音修正)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/root/group-shared/models/base_models/Qwen3-32B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "for _ in range(8):\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8209baeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9f154ddaeb44f0ae7755c32300c3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_my_name = \"/root/group-shared/jrc/ppo-test/models/train_8gpu_param_offload_left_padding/checkpoint-1-0\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_my_name)\n",
    "model_my= AutoModelForCausalLM.from_pretrained(\n",
    "    model_my_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_my_inputs = tokenizer([text], return_tensors=\"pt\").to(model_my.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92a3444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n"
     ]
    }
   ],
   "source": [
    "for _ in range(16):\n",
    "    # conduct text completion\n",
    "    generated_ids = model_my.generate(\n",
    "        **model_my_inputs,\n",
    "        max_new_tokens=32768,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_my_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa2c5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a655f6bcb944e9964785ae9d781124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: **下一步跟读的句子：** `apple` (仅单词)\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 根据学生当前的学习状态、教学目标和纠错策略，我们应优先解决上一次练习中出现的错误类型B（单词发音错误，字母发音正确）——即学生在“/æ/ /æ/ apple”中对单词“apple”的辅音/p/发音有瑕疵。\n",
      "\n",
      "因此，下一步教学应聚焦于目标单词的发音练习。\n",
      "\n",
      "**下一步跟读的句子：**  \n",
      "**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 根据学生当前学习状态和错误类型（**错误类型B**：单词发音错，字母发音对），下一步应聚焦于**目标单词练习**，即强化单词 **apple** 的正确发音。\n",
      "\n",
      "因此，下一步教学指令为：\n",
      "\n",
      "**apple**\n",
      "thinking content: \n",
      "content: 根据学生当前的学习状态和表现（上次练习内容为“/æ/ /æ/ apple”，评分B，错误集中在单词“apple”中的辅音/p/发音），下一步应聚焦**单词发音的精准练习**，尤其是“apple”中/p/的发音。\n",
      "\n",
      "结合教学控制参数，我们选择**单词专项练习**，并控制练习强度，避免疲劳。\n",
      "\n",
      "**下一步跟读的句子：**  \n",
      "**apple**\n",
      "thinking content: \n",
      "content: 根据学生当前的学习记录和教学目标，她在“音素-单词关联”阶段表现尚可，但“apple”中的辅音/p/发音仍有瑕疵。这属于**错误类型B**，应优先聚焦单词发音的练习。\n",
      "\n",
      "下一步教学指令为：\n",
      "\n",
      "**\"apple\" (仅单词，重点练习/p/的正确发音)**\n",
      "thinking content: \n",
      "content: **下一步跟读的句子：** `apple` (仅单词)\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple (请跟读)\n",
      "thinking content: \n",
      "content: **下一步跟读的句子：** `apple` (仅单词)\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 根据学生当前的学习状态、表现记录及教学控制参数，我们发现学生在**音素-单词关联**阶段已能正确发出音素 /æ/，但在单词 **apple** 的完整发音中，**辅音 /p/** 的发音仍存在问题，属于**错误类型B**。\n",
      "\n",
      "因此，下一步教学应聚焦于**单词 'apple' 的发音练习**，以帮助学生建立更清晰的语音意识和发音准确性。\n",
      "\n",
      "**下一步跟读的句子：**  \n",
      "**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n"
     ]
    }
   ],
   "source": [
    "model_my_name = \"/root/group-shared/jrc/ppo-test/models/train_4gpu_liger\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_my_name)\n",
    "model_my_liger= AutoModelForCausalLM.from_pretrained(\n",
    "    model_my_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_my_liger_inputs = tokenizer([text], return_tensors=\"pt\").to(model_my_liger.device)\n",
    "\n",
    "for _ in range(16):\n",
    "    # conduct text completion\n",
    "    generated_ids = model_my_liger.generate(\n",
    "        **model_my_liger_inputs,\n",
    "        max_new_tokens=32768\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_my_liger_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2895b340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 00:16:44,327 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 00:16:45,581 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"How are you today?\"\n",
    "text1 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "text2 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you, today?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text1, text2], padding=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "lm_backbone = getattr(model, model.base_model_prefix)\n",
    "# conduct text completion\n",
    "output = lm_backbone(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768,\n",
    "    return_dict_in_generate=True,\n",
    "    return_dict=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5968839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3Model(\n",
      "  (embed_tokens): Embedding(151936, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen3DecoderLayer(\n",
      "      (self_attn): Qwen3Attention(\n",
      "        (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Qwen3MLP(\n",
      "        (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (rotary_emb): Qwen3RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lm_backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a26fb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 14, 1024])\n",
      "29\n",
      "torch.Size([2, 14])\n"
     ]
    }
   ],
   "source": [
    "print(output.hidden_states[-1].shape)\n",
    "print(len(output.hidden_states))\n",
    "print(model_inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4e0b6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935cee6be92544788c92cd0aca402e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/root/group-shared/models/base_models/Qwen3-32B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"How are you today?\"\n",
    "text1 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "text2 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you, today?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e0ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text1, text2], padding=True, padding_side=\"left\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c248123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151643, 151643, 151644,    872,    198,   4340,    525,    498,     30,\n",
      "         151645,    198, 151644,  77091,    198],\n",
      "        [151644,    872,    198,   4340,    525,    498,     11,   3351,     30,\n",
      "         151645,    198, 151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "tensor([[151643, 151643, 151644,    872,    198,   4340,    525,    498,     30,\n",
      "         151645,    198, 151644,  77091,    198, 151667,    198,  10061,    752,\n",
      "           1744,    911,   1246,    311,   5889,    311,    419,  11657,  42113,\n",
      "           2146,   5338,     11,    358,   1184,    311,  24645,    279,   3405,\n",
      "            911,    847,   1632,  32751,     13,   8704,    358,   2776,    458,\n",
      "          15235,     11,    358,   1513,    944,   3139,  21261,    304,    279,\n",
      "           1852,   1616,  12677,    653,     11,    714,    358,   1265,    387,\n",
      "          10745,   1393,   2058,   1660,  22570,    382,     40,   1265,   2908,\n",
      "            279,   1196,    594,  13057,    481,    807,   2299,   4363,   1660,\n",
      "          35197,  11657,    323,   1366,    311,   1191,    264,  10435,     13,\n",
      "           3017,   2033,   1265,    387,   8205,    323,  41192,     11,  25836,\n",
      "           4623,  16230,    382,     40,   1265,   8172,  47848,    911,    847,\n",
      "           6993,    448,   6825,    264,  35287,  16566,     13,  10696,   6286,\n",
      "            847,  16928,    323,  35132,    369,  10476,     30,  13655,    432,\n",
      "           3100,    323,   6785,    382,  13394,     11,   1744,    911,  12752,\n",
      "           7185,  13518,    433,    481,    304,   1657,  26735,     11,  10161,\n",
      "            330,   5158,    525,    498,   7521,    374,    264,   5297,  42113,\n",
      "           4751,   1091,    264,  23141,  25893,     13,    358,   1265,   5889,\n",
      "            304,   3093,    448,    264,  11657,     11,   7517,   1663,  16232,\n",
      "            382,     40,   1184,    311,   5648,    916,   5689,    415,   1095,\n",
      "           2513,   1393,   2058,   1660,  13210,     13,   6771,    752,  10770,\n",
      "            264,   2033,    429,    594,   2176,  10745,    911,    847,   6993,\n",
      "            323,   8205,    304,  16232,    624, 151668,    271,  13048,   1052,\n",
      "              0,   5976,    358,   1513,    944,   3139,  15650,   5008,   1075,\n",
      "          12677,    653,     11,    358,   2776,   2677,  12035,    311,   6236,\n",
      "            323,   1492,    700,      0,    220,    149,    102,      7, 145101,\n",
      "         145219, 145101, 139513,      8,    151,    114,    358,   2948,   6832,\n",
      "            501,   2513,   1526,   1039,  20959,     13,   3555,    594,    389,\n",
      "            697,   3971,   3351,     30, 151645, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643],\n",
      "        [151644,    872,    198,   4340,    525,    498,     11,   3351,     30,\n",
      "         151645,    198, 151644,  77091,    198, 151667,    198,  10061,    752,\n",
      "          15516,   2908,   1246,    311,   5889,    311,    419,   3405,     13,\n",
      "           5512,     11,    358,   1184,    311,  24645,    429,    358,   1513,\n",
      "            944,    614,  15650,    476,  24875,    304,    279,   1616,  12677,\n",
      "            653,     13,   1988,    358,   1265,   2058,   5889,    304,    264,\n",
      "          11657,    323,  10950,  11566,    429,  31995,    264,   5810,  10435,\n",
      "           6396,    382,     40,    646,   1191,    553,   9355,  27798,    847,\n",
      "           1482,   2639,    438,    458,  15235,   1849,    429,   3171,    944,\n",
      "           3139,  21261,     13,   4354,     11,    358,    646,   2058,   3158,\n",
      "          35132,    369,   1039,  10435,    323,  38275,    311,   1492,     13,\n",
      "           1096,   5486,  31995,  47848,   1393,  10282,    279,  16230,  22570,\n",
      "            382,     40,   1265,   1083,   2908,    279,   7517,   1663,   2266,\n",
      "             13,   8704,    279,   1196,   4588,    911,    847,   1632,  32751,\n",
      "           1156,     11,    358,    646,   9142,  38411,    311,  10161,    911,\n",
      "            862,   1632,  32751,    304,    470,     13,   1096,  11450,    264,\n",
      "          23831,     11,   1378,  26798,  10435,    382,   5050,   2033,   1265,\n",
      "            387,   8205,    714,   6584,     11,   9027,  22011,   2734,    304,\n",
      "           1660,  10950,   1393,   1660,   2797,    911,    847,   6993,     13,\n",
      "            358,   1366,    311,   5648,    894,  37209,  24154,    911,   3432,\n",
      "          15650,     11,   1393,   2058,   3259,    279,  10435,   2666,   5810,\n",
      "            323,   5486,    480,    382,     40,   3278,   2506,    279,  16232,\n",
      "          11657,    323,   7517,   1663,     11,  30426,  38432,  10916,   4128,\n",
      "             13,    576,   5795,    374,    311,    387,  10950,    323,  22570,\n",
      "           1393,  20337,  27231,    911,    847,  16928,    382,  23949,     11,\n",
      "            358,   1366,    311,   1281,   2704,    847,   2033,    374,  63594,\n",
      "            714,   4583,     11,  27020,   2176,    847,   1482,   2639,    323,\n",
      "           9027,   2734,    304,  14354,    279,  10435,    304,    264,  22414,\n",
      "           1616,    624, 151668,    271,  13048,   1052,      0,  26525,    232,\n",
      "           5976,    358,   1513,    944,   3139,  15650,   5008,   1075,  12677,\n",
      "            653,    320,     40,   2776,    803,    315,    264,  22208,   6540,\n",
      "          60812,  48643,    358,   2776,   2677,  12035,    311,   6236,    323,\n",
      "           1492,    700,     13,   2585,    525,    498,   8266,   3351,     30,\n",
      "            358,   4172,   2948,    311,   6723,    911,    697,   1899,    323,\n",
      "           1490,   1246,    358,    646,   7789,    498,      0,  11162,    234,\n",
      "            253, 151645]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model_inputs)\n",
    "print(output.sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4ea678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Let me think about how to respond to this friendly greeting...\n",
      "\n",
      "First, I need to acknowledge the question about my well-being. Since I'm an AI, I don't experience emotions in the same way humans do, but I should be honest while still being engaging.\n",
      "\n",
      "I should consider the user's perspective - they're likely being genuinely friendly and want to start a conversation. My response should be warm and inviting, encouraging further interaction.\n",
      "\n",
      "I should balance honesty about my nature with creating a welcoming atmosphere. Maybe mention my capabilities and enthusiasm for helping? Keep it light and positive.\n",
      "\n",
      "Also, think about cultural appropriateness - in many cultures, asking \"how are you?\" is a standard greeting rather than a literal inquiry. I should respond in kind with a friendly, conversational tone.\n",
      "\n",
      "I need to avoid overcomplicating things while still being authentic. Let me craft a response that's both honest about my nature and warm in tone.\n",
      "</think>\n",
      "content: Hi there! While I don't experience feelings quite like humans do, I'm always excited to chat and help out! ٩(◕‿◕｡)۶ I love learning new things through our conversations. What's on your mind today?\n",
      "thinking content: <think>\n",
      "Let me carefully consider how to respond to this question. First, I need to acknowledge that I don't have feelings or consciousness in the way humans do. But I should still respond in a friendly and helpful manner that maintains a natural conversation flow.\n",
      "\n",
      "I can start by clearly stating my current status as an AI system that doesn't experience emotions. However, I can still express enthusiasm for our conversation and willingness to help. This approach maintains honesty while keeping the interaction engaging.\n",
      "\n",
      "I should also consider the conversational context. Since the user asked about my well-being first, I can transition smoothly to asking about their well-being in return. This creates a balanced, two-way conversation.\n",
      "\n",
      "My response should be warm but professional, showing genuine interest in being helpful while being clear about my nature. I want to avoid any misleading implications about having feelings, while still making the conversation feel natural and approachable.\n",
      "\n",
      "I'll keep the tone friendly and conversational, avoiding overly technical language. The goal is to be helpful and engaging while maintaining transparency about my capabilities.\n",
      "\n",
      "Finally, I want to make sure my response is concise but complete, addressing both my current status and showing interest in continuing the conversation in a meaningful way.\n",
      "</think>\n",
      "content: Hi there! 😊 While I don't experience feelings quite like humans do (I'm more of a curious knowledge enthusiast!), I'm always excited to chat and help out. How are you feeling today? I'd love to hear about your day and see how I can assist you! 🌟\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    output_ids = output.sequences[i][len(model_inputs.input_ids[i]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "992b213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([50.2083,    -inf,    -inf,    -inf,    -inf], device='cuda:0'),\n",
      "indices=tensor([151667,      2,      0,      3,      1], device='cuda:0'))\n",
      "torch.return_types.topk(\n",
      "values=tensor([71.2500,    -inf,    -inf,    -inf,    -inf], device='cuda:0'),\n",
      "indices=tensor([198,   2,   0,   3,   1], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.topk(output.scores[0][0],5))\n",
    "print(torch.topk(output.scores[1][0],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba6ae9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,    872,    198,   4340,    525,    498,     30, 151645,    198,\n",
      "         151644,  77091,    198, 151643, 151643],\n",
      "        [151644,    872,    198,   4340,    525,    498,     11,   3351,     30,\n",
      "         151645,    198, 151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "tensor([[  3838,    264,  11657,   3405,      0,    358,   2776,   1588,    323,\n",
      "           5527,    311,   6236,     13,   5976,    358,   1513,    944,   3139,\n",
      "          15650,    304,    279,   1616,  12677,    653,     11,    358,   2776,\n",
      "           2677,  12035,    311,  16579,    304,  10435,    323,   1492,    448,\n",
      "           8820,    498,   1184,     13,   3555,    594,    389,    697,   3971,\n",
      "           3351,     30,  26525,    232,    271, 151644, 151644, 151644, 151644,\n",
      "         151644, 151644, 151644, 151644,    198, 151644,    271,     40,   2776,\n",
      "           1661,      0,  11114,    369,  10161,      0,   2585,    525,    498,\n",
      "           3730,   3351,     30,  26525,    232, 151645, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643],\n",
      "        [151667,    198,  10061,    752,   1744,    911,   1246,    311,   5889,\n",
      "            311,    419,  11657,  42113,     13,   5512,     11,    358,   1184,\n",
      "            311,   2908,    279,   6993,    315,    847,  13885,    481,    358,\n",
      "           1513,    944,   3139,  21261,    304,    279,   1616,  12677,    653,\n",
      "             11,    714,    358,    646,   7838,  16579,    304,  42666,  10435,\n",
      "            911,   1493,  13347,     13,   4710,   1986,   3405,    911,    847,\n",
      "           1632,  32751,    374,   5008,   4185,    304,   3738,  21880,     11,\n",
      "           3545,  13480,    438,    264,   3590,   9853,  64121,     13,    358,\n",
      "           1265,  24645,    419,   1393,   1660,  10745,    911,    847,   4911,\n",
      "           2309,    438,    458,  15235,     13,   1084,    594,   2989,    311,\n",
      "          10306,  53248,   1393,   2058,   1660,  22570,    323,  10950,    382,\n",
      "             40,   1410,  13186,   5257,  25941,    304,    847,   2033,    481,\n",
      "           8365,  30587,    389,    279,   6993,    315,  24875,     11,    279,\n",
      "           7428,    315,    847,  13885,     11,    476,   1246,    358,   1882,\n",
      "           1995,     13,   1988,    358,   1265,   2506,    432,  44345,    323,\n",
      "           1351,  15086,     13,    576,   1376,    374,    311,    387,  17821,\n",
      "            911,    847,  16928,   1393,   9664,   5486,    480,    382,     40,\n",
      "           1265,   1083,   2908,    279,   2266,    481,    419,    374,   4363,\n",
      "            279,   7167,    315,    264,  10435,     11,    773,    847,   2033,\n",
      "           1265,    387,   1787,  83075,    311,  14907,   4623,  21276,     13,\n",
      "          10696,    358,    646,   3158,  40228,    911,    279,   1196,    594,\n",
      "           1632,  32751,    304,    470,     11,   6825,    264,    803,  23831,\n",
      "          16230,    382,     40,   1184,    311,    387,  16585,    537,    311,\n",
      "            916,   5689,  48795,   2513,    448,  40803,   3091,    819,     13,\n",
      "            362,   4285,     11,  10745,   2033,    429,  44410,   4623,  10435,\n",
      "           1035,    387,   1429,   7373,   1588,     13,   6771,    594,   5244,\n",
      "            389,   1660,  10950,   1393,  20337,  53248,    911,    847,   6993,\n",
      "            438,    458,  15235,  17847,    624, 151668,    271,  12658,    369,\n",
      "          10161,      0,    358,   2776,  30201,   1632,    323,   5527,    311,\n",
      "           6236,    476,   1492,    448,   8820,    498,   1184,     13,    358,\n",
      "           1513,    944,   3139,  15650,   5008,   1075,  12677,    653,     11,\n",
      "            714,    358,  35197,   4669,   1039,  20959,      0,   2585,    525,\n",
      "            498,   8266,   3351,     30,    358,   4172,   2948,    311,   6723,\n",
      "           1128,    594,    389,    697,   3971,     13,  26525,    232, 151645]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer([text1, text2], padding=True, padding_side=\"right\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ") \n",
    "print(model_inputs)\n",
    "print(output.sequences[:,model_inputs.input_ids.shape[1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e965fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: What a friendly question! I'm here and ready to chat. While I don't experience feelings in the way humans do, I'm always excited to engage in conversation and help with whatever you need. What's on your mind today? 😊\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I'm good! Thanks for asking! How are you doing today? 😊\n",
      "thinking content: <think>\n",
      "Let me think about how to respond to this friendly greeting. First, I need to consider the nature of my existence - I don't experience emotions in the way humans do, but I can certainly engage in thoughtful conversation about these topics. \n",
      "\n",
      "This question about my well-being is quite common in human interactions, often serving as a social icebreaker. I should acknowledge this while being honest about my unique position as an AI. It's important to maintain authenticity while still being engaging and helpful.\n",
      "\n",
      "I could explore various angles in my response - perhaps touching on the nature of consciousness, the purpose of my existence, or how I process information. But I should keep it grounded and relatable. The key is to be transparent about my capabilities while remaining approachable.\n",
      "\n",
      "I should also consider the context - this is likely the beginning of a conversation, so my response should be open-ended to encourage further dialogue. Maybe I can express curiosity about the user's well-being in return, creating a more balanced interaction.\n",
      "\n",
      "I need to be careful not to overcomplicate things with philosophical musings. A simple, honest response that invites further conversation would be most effective here. Let's focus on being helpful while maintaining authenticity about my nature as an AI assistant.\n",
      "</think>\n",
      "content: Thanks for asking! I'm functioning well and ready to chat or help with whatever you need. I don't experience feelings quite like humans do, but I genuinely enjoy our conversations! How are you feeling today? I'd love to hear what's on your mind. 😊\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    output_ids = output.sequences[i][len(model_inputs.input_ids[i]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "933b2ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([24.1667, 22.9167, 22.5000, 21.8750, 21.0417], device='cuda:0'),\n",
      "indices=tensor([ 4340,    40,  3838,  9707, 13048], device='cuda:0'))\n",
      "torch.return_types.topk(\n",
      "values=tensor([51.0417,    -inf,    -inf,    -inf,    -inf], device='cuda:0'),\n",
      "indices=tensor([151667,      2,      0,      3,      1], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.topk(output.scores[0][0],5))\n",
    "print(torch.topk(output.scores[0][1],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f148854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aixue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
