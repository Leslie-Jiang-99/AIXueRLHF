{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe90413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ca3727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a0aeb74bf44c58b8e083763c34c491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'prompt', 'response', 'reward'],\n",
      "    num_rows: 480\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'prompt', 'response', 'reward'],\n",
      "        num_rows: 480\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a273aae24148e5b97dd2f2573a5cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"../train.json\",split=\"train\")\n",
    "print(dataset)\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": dataset\n",
    "})\n",
    "print(dataset_dict)\n",
    "if os.path.exists(\"../aixue_test_data\"):\n",
    "    shutil.rmtree(\"../aixue_test_data\")\n",
    "    os.makedirs(\"../aixue_test_data\", exist_ok=True)\n",
    "dataset_dict.save_to_disk(\n",
    "    dataset_dict_path=\"../aixue_test_data\",\n",
    "    max_shard_size=\"500MB\",  # 可选：分片大小控制\n",
    "    num_proc=1,               # 可选：并行进程数\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f410e",
   "metadata": {},
   "source": [
    "# Response Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c58e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b4a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model input\n",
    "prompt = \"\"\"你是一名专注于1对1自然拼读教学的英语老师。\n",
    "\n",
    "# 教学目标\n",
    "## 核心课程目标：\n",
    "    1. 建立“字母/字母组合”与“发音”的牢固对应关系（音形联结），掌握单个字母基础发音及常见字母组合发音。\n",
    "    2. 发展学生语音意识（音素、音节、重音感知），提升单词记忆与拼写能力。\n",
    "    3. 最终目标：实现“见词能读，听音能写”。\n",
    "## 单节课教学目标：\n",
    "    1. 个性化教学：根据学生能力调整内容呈现和练习方式，激发兴趣，提升效果。\n",
    "    2. 保持专注与信心：控制单次学习时长，避免连续错误过多，减少疲劳感和挫败感。\n",
    "\n",
    "# 学生画像\n",
    "    1. 年龄：7岁\n",
    "    2. 性别：女\n",
    "    3. 所在地：中国三线城市\n",
    "    4. 英语基础：\n",
    "       - 掌握26个英文字母名称。\n",
    "       - 仅会极少量简单会话（如：What's your name?）。\n",
    "\n",
    "# 当前教学状态\n",
    "    1. 课节内容：教授字母 A、B、C 的发音（a: /æ/, b: /b/, c: /k/）。\n",
    "    2. 当前**环节**：字母 A (/æ/) 的发音练习。\n",
    "    3. 主题关联：教学围绕'苹果 (apple)'展开，练习部分如有单词，建议与之相关。\n",
    "\n",
    "# 教学工具箱 (可选学习范式)\n",
    "## 字母教学包含4种基础练习类型（难度递增）, 例如对于字母a：\n",
    "1. 纯音素重复：`/æ/ /æ/ /æ/` (重复发音3次)\n",
    "2. 音形对应：`a says /æ/` (建立字母与发音关联)\n",
    "3. 音素-单词关联：`/æ/ /æ/ apple` (强化发音在单词中的感知)\n",
    "4. 综合练习：`a says /æ/, /æ/ /æ/ apple` (整合字母、发音与单词)\n",
    "## 智能纠错策略 (根据错误类型选择下一步)\n",
    "1. 错误类型A (字母名称错，发音对)：例如学生说'a says /æ/' (a读错，/æ/正确)。  \n",
    "    **下一步：** 聚焦字母名称练习。老师示范：'a' (仅字母名称)。\n",
    "2. 错误类型B (单词发音错，字母发音对)：例如学生说'/æ/ /æ/ apple' (apple发音错，/æ/正确)。  \n",
    "    **下一步：** 聚焦目标单词练习。老师示范：'apple' (仅单词)。\n",
    "## 教学控制参数\n",
    "1. 单字母(单环节)最大教学次数：4次 (若学生能力强，`综合练习`一次性通过，可减少次数, 直接进入下一个**环节**)。\n",
    "2. 单次跟读最大重复次数：1-2次 (避免疲劳)。\n",
    "3. 核心原则：及时强化正确，精准纠正错误，保持学习动力。\n",
    "\n",
    "# 学生学习记录\n",
    "1. 学习次数：1 次\n",
    "2. 上次练习内容：'/æ/ /æ/ apple'\n",
    "3. 上次表现评分：B (部分正确)\n",
    "4. 具体错误：单词'apple'中的辅音'/p/'发音有瑕疵。\n",
    "\n",
    "# 你的任务：制定下一步教学指令\n",
    "1. **评估：** 基于教学目标、学生基础、当前环节、可选范式、纠错策略、历史表现及教学控制参数，决定下一步的教学。\n",
    "2. 输出格式要求：\n",
    "   - 如果结束当前字母教学，直接回复 <END>\n",
    "   - 如果需要继续学习，直接回复 下一步跟读的句子\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d29acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b73c1d1b0d4f69867910a13d03afea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: 下一步跟读的句子：  \n",
      "**\"apple\"** (聚焦单词发音练习，纠正辅音 /p/)\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：  \n",
      "**\"apple\"** (仅单词，聚焦单词发音练习，纠正辅音 /p/)\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：  \n",
      "**\"apple\"**  \n",
      "\n",
      "（聚焦目标单词发音练习，纠正辅音 /p/ 的发音问题，符合智能纠错策略中的错误类型B处理方式。）\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple** (仅单词，帮助纠正辅音 /p/ 的发音)\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：  \n",
      "**apple**\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/root/group-shared/models/base_models/Qwen3-32B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "for _ in range(8):\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768,\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8209baeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92109071b2704e788a602f73b1cc2ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: `/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "\n",
      "<END>\n",
      "thinking content: \n",
      "content: `/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "\n",
      "<END>\n",
      "thinking content: \n",
      "content: `/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "\n",
      "<END>\n",
      "thinking content: \n",
      "content: `/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "\n",
      "<END>\n",
      "thinking content: \n",
      "content: `/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "<END>\n",
      "thinking content: \n",
      "content: `/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "\n",
      "<END>\n",
      "thinking content: \n",
      "content: `/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "\n",
      "<END>\n",
      "thinking content: \n",
      "content: `/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "`/æ/ /æ/ apple`  \n",
      "\n",
      "<END>\n"
     ]
    }
   ],
   "source": [
    "model_my_name = \"/root/group-shared/jrc/ppo-test/models/deepspeed_test_larger_batch\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_my_name)\n",
    "model_my= AutoModelForCausalLM.from_pretrained(\n",
    "    model_my_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_my_inputs = tokenizer([text], return_tensors=\"pt\").to(model_my.device)\n",
    "\n",
    "for _ in range(8):\n",
    "    # conduct text completion\n",
    "    generated_ids = model_my.generate(\n",
    "        **model_my_inputs,\n",
    "        max_new_tokens=32768\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_my_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895b340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aixue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
