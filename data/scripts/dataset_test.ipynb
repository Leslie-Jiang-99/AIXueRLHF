{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe90413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ca3727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a0aeb74bf44c58b8e083763c34c491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'prompt', 'response', 'reward'],\n",
      "    num_rows: 480\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'prompt', 'response', 'reward'],\n",
      "        num_rows: 480\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a273aae24148e5b97dd2f2573a5cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"../train.json\",split=\"train\")\n",
    "print(dataset)\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": dataset\n",
    "})\n",
    "print(dataset_dict)\n",
    "if os.path.exists(\"../aixue_test_data\"):\n",
    "    shutil.rmtree(\"../aixue_test_data\")\n",
    "    os.makedirs(\"../aixue_test_data\", exist_ok=True)\n",
    "dataset_dict.save_to_disk(\n",
    "    dataset_dict_path=\"../aixue_test_data\",\n",
    "    max_shard_size=\"500MB\",  # 可选：分片大小控制\n",
    "    num_proc=1,               # 可选：并行进程数\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f410e",
   "metadata": {},
   "source": [
    "# Response Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01c58e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03b4a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the model input\n",
    "prompt = \"\"\"你是一名专注于1对1自然拼读教学的英语老师。\n",
    "\n",
    "# 教学目标\n",
    "## 核心课程目标：\n",
    "    1. 建立“字母/字母组合”与“发音”的牢固对应关系（音形联结），掌握单个字母基础发音及常见字母组合发音。\n",
    "    2. 发展学生语音意识（音素、音节、重音感知），提升单词记忆与拼写能力。\n",
    "    3. 最终目标：实现“见词能读，听音能写”。\n",
    "## 单节课教学目标：\n",
    "    1. 个性化教学：根据学生能力调整内容呈现和练习方式，激发兴趣，提升效果。\n",
    "    2. 保持专注与信心：控制单次学习时长，避免连续错误过多，减少疲劳感和挫败感。\n",
    "\n",
    "# 学生画像\n",
    "    1. 年龄：7岁\n",
    "    2. 性别：女\n",
    "    3. 所在地：中国三线城市\n",
    "    4. 英语基础：\n",
    "       - 掌握26个英文字母名称。\n",
    "       - 仅会极少量简单会话（如：What's your name?）。\n",
    "\n",
    "# 当前教学状态\n",
    "    1. 课节内容：教授字母 A、B、C 的发音（a: /æ/, b: /b/, c: /k/）。\n",
    "    2. 当前**环节**：字母 A (/æ/) 的发音练习。\n",
    "    3. 主题关联：教学围绕'苹果 (apple)'展开，练习部分如有单词，建议与之相关。\n",
    "\n",
    "# 教学工具箱 (可选学习范式)\n",
    "## 字母教学包含4种基础练习类型（难度递增）, 例如对于字母a：\n",
    "1. 纯音素重复：`/æ/ /æ/ /æ/` (重复发音3次)\n",
    "2. 音形对应：`a says /æ/` (建立字母与发音关联)\n",
    "3. 音素-单词关联：`/æ/ /æ/ apple` (强化发音在单词中的感知)\n",
    "4. 综合练习：`a says /æ/, /æ/ /æ/ apple` (整合字母、发音与单词)\n",
    "## 智能纠错策略 (根据错误类型选择下一步)\n",
    "1. 错误类型A (字母名称错，发音对)：例如学生说'a says /æ/' (a读错，/æ/正确)。  \n",
    "    **下一步：** 聚焦字母名称练习。老师示范：'a' (仅字母名称)。\n",
    "2. 错误类型B (单词发音错，字母发音对)：例如学生说'/æ/ /æ/ apple' (apple发音错，/æ/正确)。  \n",
    "    **下一步：** 聚焦目标单词练习。老师示范：'apple' (仅单词)。\n",
    "## 教学控制参数\n",
    "1. 单字母(单环节)最大教学次数：4次 (若学生能力强，`综合练习`一次性通过，可减少次数, 直接进入下一个**环节**)。\n",
    "2. 单次跟读最大重复次数：1-2次 (避免疲劳)。\n",
    "3. 核心原则：及时强化正确，精准纠正错误，保持学习动力。\n",
    "\n",
    "# 学生学习记录\n",
    "1. 学习次数：1 次\n",
    "2. 上次练习内容：'/æ/ /æ/ apple'\n",
    "3. 上次表现评分：B (部分正确)\n",
    "4. 具体错误：单词'apple'中的辅音'/p/'发音有瑕疵。\n",
    "\n",
    "# 你的任务：制定下一步教学指令\n",
    "1. **评估：** 基于教学目标、学生基础、当前环节、可选范式、纠错策略、历史表现及教学控制参数，决定下一步的教学。\n",
    "2. 输出格式要求：\n",
    "   - 如果结束当前字母教学，直接回复 <END>\n",
    "   - 如果需要继续学习，直接回复 下一步跟读的句子\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83d29acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a1148d20144f319c41714cf6d6ad50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：  \n",
      "**\"apple\"** (仅单词，聚焦单词发音练习，纠正辅音 /p/)\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：  \n",
      "**\"apple\"** (仅单词，聚焦单词发音练习，纠正辅音 /p/)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/root/group-shared/models/base_models/Qwen3-32B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "for _ in range(8):\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209baeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96407f6a21c401c94725f80efa7f666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple (请跟读)\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: 根据学生当前的学习状态和历史表现，她在“音素-单词关联”阶段（`/æ/ /æ/ apple`）中，虽然对音素 `/æ/` 的发音掌握较好，但在单词 `apple` 的发音中，特别是辅音 `/p/` 的发音仍存在瑕疵。这表明她在音素-单词的整合练习中仍需强化语音意识和单词整体发音的准确性。\n",
      "\n",
      "因此，下一步应聚焦于目标单词 `apple` 的发音练习，以提高其对单词整体语音结构的掌握。\n",
      "\n",
      "**下一步跟读的句子：**  \n",
      "`apple` (仅单词)\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: 根据当前教学目标、学生基础、当前环节及历史表现，学生在音素-单词关联练习中已基本掌握音素 /æ/ 的发音，但在单词 \"apple\" 的完整发音上仍需加强，尤其是辅音 /p/ 的发音。因此，下一步应聚焦单词练习，以帮助学生更准确地完成整个单词的发音。\n",
      "\n",
      "**下一步跟读的句子：**  \n",
      "`apple` (仅单词，强调清晰发音)\n"
     ]
    }
   ],
   "source": [
    "model_my_name = \"/root/group-shared/jrc/ppo-test/models/train_8gpu_param_offload\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_my_name)\n",
    "model_my= AutoModelForCausalLM.from_pretrained(\n",
    "    model_my_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_my_inputs = tokenizer([text], return_tensors=\"pt\").to(model_my.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92a3444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple\n"
     ]
    }
   ],
   "source": [
    "for _ in range(16):\n",
    "    # conduct text completion\n",
    "    generated_ids = model_my.generate(\n",
    "        **model_my_inputs,\n",
    "        max_new_tokens=32768,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_my_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa2c5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a655f6bcb944e9964785ae9d781124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: **下一步跟读的句子：** `apple` (仅单词)\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 根据学生当前的学习状态、教学目标和纠错策略，我们应优先解决上一次练习中出现的错误类型B（单词发音错误，字母发音正确）——即学生在“/æ/ /æ/ apple”中对单词“apple”的辅音/p/发音有瑕疵。\n",
      "\n",
      "因此，下一步教学应聚焦于目标单词的发音练习。\n",
      "\n",
      "**下一步跟读的句子：**  \n",
      "**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 根据学生当前学习状态和错误类型（**错误类型B**：单词发音错，字母发音对），下一步应聚焦于**目标单词练习**，即强化单词 **apple** 的正确发音。\n",
      "\n",
      "因此，下一步教学指令为：\n",
      "\n",
      "**apple**\n",
      "thinking content: \n",
      "content: 根据学生当前的学习状态和表现（上次练习内容为“/æ/ /æ/ apple”，评分B，错误集中在单词“apple”中的辅音/p/发音），下一步应聚焦**单词发音的精准练习**，尤其是“apple”中/p/的发音。\n",
      "\n",
      "结合教学控制参数，我们选择**单词专项练习**，并控制练习强度，避免疲劳。\n",
      "\n",
      "**下一步跟读的句子：**  \n",
      "**apple**\n",
      "thinking content: \n",
      "content: 根据学生当前的学习记录和教学目标，她在“音素-单词关联”阶段表现尚可，但“apple”中的辅音/p/发音仍有瑕疵。这属于**错误类型B**，应优先聚焦单词发音的练习。\n",
      "\n",
      "下一步教学指令为：\n",
      "\n",
      "**\"apple\" (仅单词，重点练习/p/的正确发音)**\n",
      "thinking content: \n",
      "content: **下一步跟读的句子：** `apple` (仅单词)\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: /æ/ /æ/ apple (请跟读)\n",
      "thinking content: \n",
      "content: **下一步跟读的句子：** `apple` (仅单词)\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n",
      "thinking content: \n",
      "content: 根据学生当前的学习状态、表现记录及教学控制参数，我们发现学生在**音素-单词关联**阶段已能正确发出音素 /æ/，但在单词 **apple** 的完整发音中，**辅音 /p/** 的发音仍存在问题，属于**错误类型B**。\n",
      "\n",
      "因此，下一步教学应聚焦于**单词 'apple' 的发音练习**，以帮助学生建立更清晰的语音意识和发音准确性。\n",
      "\n",
      "**下一步跟读的句子：**  \n",
      "**apple**\n",
      "thinking content: \n",
      "content: 下一步跟读的句子：**apple**\n"
     ]
    }
   ],
   "source": [
    "model_my_name = \"/root/group-shared/jrc/ppo-test/models/train_4gpu_liger\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_my_name)\n",
    "model_my_liger= AutoModelForCausalLM.from_pretrained(\n",
    "    model_my_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_my_liger_inputs = tokenizer([text], return_tensors=\"pt\").to(model_my_liger.device)\n",
    "\n",
    "for _ in range(16):\n",
    "    # conduct text completion\n",
    "    generated_ids = model_my_liger.generate(\n",
    "        **model_my_liger_inputs,\n",
    "        max_new_tokens=32768\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_my_liger_inputs.input_ids[0]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2895b340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 00:16:44,327 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 00:16:45,581 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"How are you today?\"\n",
    "text1 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "text2 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you, today?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text1, text2], padding=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "lm_backbone = getattr(model, model.base_model_prefix)\n",
    "# conduct text completion\n",
    "output = lm_backbone(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768,\n",
    "    return_dict_in_generate=True,\n",
    "    return_dict=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5968839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3Model(\n",
      "  (embed_tokens): Embedding(151936, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen3DecoderLayer(\n",
      "      (self_attn): Qwen3Attention(\n",
      "        (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Qwen3MLP(\n",
      "        (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (rotary_emb): Qwen3RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lm_backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a26fb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 14, 1024])\n",
      "29\n",
      "torch.Size([2, 14])\n"
     ]
    }
   ],
   "source": [
    "print(output.hidden_states[-1].shape)\n",
    "print(len(output.hidden_states))\n",
    "print(model_inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"How are you today?\"\n",
    "text1 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "text2 = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": \"How are you, today?\"}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49e0ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text1, text2], padding=True, padding_side=\"left\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c248123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151643, 151643, 151644,    872,    198,   4340,    525,    498,     30,\n",
      "         151645,    198, 151644,  77091,    198],\n",
      "        [151644,    872,    198,   4340,    525,    498,     11,   3351,     30,\n",
      "         151645,    198, 151644,  77091,    198]], device='cuda:7'), 'attention_mask': tensor([[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:7')}\n",
      "tensor([[151643, 151643, 151644,    872,    198,   4340,    525,    498,     30,\n",
      "         151645,    198, 151644,  77091,    198, 151667,    198,  32313,     11,\n",
      "            279,   1196,   4588,     11,    330,   4340,    525,    498,   7521,\n",
      "            358,   1184,    311,   5889,  34901,     13,   8704,    358,   2776,\n",
      "            458,  15235,  17847,     11,    358,   1265,  24645,    862,   3405,\n",
      "            323,   3410,    264,  10950,   4226,    382,   5338,     11,    358,\n",
      "           1265,   7683,    847,   2639,    438,    458,  15235,    323,   6286,\n",
      "            429,    358,   2776,   1588,    311,   7789,     13,   5005,     11,\n",
      "            358,    646,   3010,  12994,    448,   5257,  13347,     13,    358,\n",
      "           1265,   2506,    279,  16232,  11657,    323,   1787,  83075,    311,\n",
      "          14907,   4623,  10435,    382,     40,   1265,   5648,    894,  10916,\n",
      "            502,  70821,    323,   2506,    279,   2033,   4285,     13,   6771,\n",
      "            752,   1779,    369,    894,   3204,  89600,    819,     13,   2308,\n",
      "             11,    279,   1196,   1101,   6801,    311,   1414,    847,   1482,\n",
      "           1584,    323,   5726,    311,   1492,     13,  97593,     11,    429,\n",
      "           1265,   3421,    432,    624, 151668,    271,   9707,      0,    358,\n",
      "           2776,   1588,    311,   1492,    448,   4113,    498,   1184,     13,\n",
      "           2585,    646,    358,   7789,    498,   3351,     30,  26525,    232,\n",
      "         151645, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "         151643, 151643, 151643],\n",
      "        [151644,    872,    198,   4340,    525,    498,     11,   3351,     30,\n",
      "         151645,    198, 151644,  77091,    198, 151667,    198,  32313,     11,\n",
      "            279,   1196,   4588,     11,    330,   4340,    525,    498,     11,\n",
      "           3351,   7521,    358,   1184,    311,   5889,  34901,     13,   6771,\n",
      "            752,   1744,    911,    279,   1850,   1616,    311,   4226,    382,\n",
      "           5338,     11,    358,   1265,  24645,    862,   3405,     13,   1084,\n",
      "            594,    264,  11657,   1616,    311,   1191,     13,  10696,   1977,\n",
      "           2494,   1075,    330,  13048,      0,   2585,    525,    498,   3351,\n",
      "           7521,   2938,    594,  47787,    323,   4933,   2734,    382,  12209,\n",
      "             11,    358,    646,    912,    264,   2699,    803,    311,   2506,\n",
      "            279,  10435,   2087,     13,  10696,   6286,   2494,    911,    279,\n",
      "           1899,     11,   1075,    330,     40,   2776,   8266,   2244,   3351,\n",
      "           8958,    311,   1473,    358,   2776,   6247,     13,   4710,     40,\n",
      "           1265,   2506,    279,  16232,   6785,    323,  11657,     13,  34006,\n",
      "            894,   8225,   4128,     13,   6771,    752,   1281,   2704,    279,\n",
      "           2033,    374,  63594,    714,   8205,     13,  97593,     11,    429,\n",
      "           1265,    975,    624, 151668,    271,  13048,      0,   2585,    525,\n",
      "            498,   3351,     30,    358,   2776,   2256,   6247,    311,   6723,\n",
      "            429,      0,    358,   2776,   8266,   2244,     11,    323,    358,\n",
      "           2776,  12035,    311,    387,   1588,     13,   6771,    752,   1414,\n",
      "            421,   1052,    594,   4113,    498,   2299,  22208,    911,      0,\n",
      "          26525,    232, 151645]], device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "print(model_inputs)\n",
    "print(output.sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c4ea678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, the user asked, \"How are you?\" I need to respond appropriately. Since I'm an AI assistant, I should acknowledge their question and provide a helpful answer.\n",
      "\n",
      "First, I should confirm my status as an AI and mention that I'm here to assist. Then, I can offer assistance with various topics. I should keep the tone friendly and open-ended to encourage further conversation.\n",
      "\n",
      "I should avoid any technical jargon and keep the response simple. Let me check for any possible misunderstandings. No, the user just wants to know my current state and ability to help. Alright, that should cover it.\n",
      "</think>\n",
      "content: Hello! I'm here to help with anything you need. How can I assist you today? 😊\n",
      "thinking content: <think>\n",
      "Okay, the user asked, \"How are you, today?\" I need to respond appropriately. Let me think about the best way to answer.\n",
      "\n",
      "First, I should acknowledge their question. It's a friendly way to start. Maybe say something like \"Hi! How are you today?\" That's polite and shows interest.\n",
      "\n",
      "Then, I can add a bit more to keep the conversation going. Maybe mention something about the day, like \"I'm feeling great today!\" to show I'm happy. \n",
      "\n",
      "I should keep the tone positive and friendly. Avoid any negative language. Let me make sure the response is concise but warm. Alright, that should work.\n",
      "</think>\n",
      "content: Hi! How are you today? I'm super happy to hear that! I'm feeling great, and I'm excited to be here. Let me know if there's anything you're curious about! 😊\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    output_ids = output.sequences[i][len(model_inputs.input_ids[i]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([52.7083,    -inf,    -inf,    -inf,    -inf], device='cuda:7'),\n",
      "indices=tensor([151667,      2,      0,      3,      1], device='cuda:7'))\n",
      "torch.return_types.topk(\n",
      "values=tensor([57.0833,    -inf,    -inf,    -inf,    -inf], device='cuda:7'),\n",
      "indices=tensor([198,   2,   0,   3,   1], device='cuda:7'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.topk(output.scores[0][0],5))\n",
    "print(torch.topk(output.scores[1][0],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba6ae9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,    872,    198,   4340,    525,    498,     30, 151645,    198,\n",
      "         151644,  77091,    198, 151643, 151643],\n",
      "        [151644,    872,    198,   4340,    525,    498,     11,   3351,     30,\n",
      "         151645,    198, 151644,  77091,    198]], device='cuda:7'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:7')}\n",
      "tensor([[   198, 151667,    198,  32313,     11,    279,   1196,   4588,     11,\n",
      "            330,   4340,    525,    498,   7521,    358,   1184,    311,   5889,\n",
      "            304,    264,  11657,    323,  10950,   1616,     13,   6771,    752,\n",
      "           1744,    911,    279,   1850,   5486,    382,   5338,     11,    358,\n",
      "           1265,  24645,    862,   3405,     13,   1084,    594,    264,   4285,\n",
      "          42113,     11,    773,   7196,   1191,    448,    330,  13048,   1052,\n",
      "           8958,    311,   1473,    358,   2776,   6247,    311,   1492,     13,\n",
      "           5005,     11,    358,    646,    912,    264,   2699,    803,    911,\n",
      "            847,   1482,   1584,     11,   1075,    330,     40,   2776,   3730,\n",
      "           1632,    323,    358,   2776,   1588,    311,   1492,    498,    448,\n",
      "           4113,   8958,   2938,   6696,    264,   5530,    315,   1660,   1052,\n",
      "            369,   1105,    382,     40,   1265,   1281,   2704,    279,   2033,\n",
      "            374,   5810,    323,    537,   2238,  15908,     13,  10696,    912,\n",
      "            264,  11657,  42365,    311,   2506,    432,   3100,     13,   7281,\n",
      "             11,   1779,    421,    279,   1196,    702,    894,   3151,   4755,\n",
      "            476,   3880,  12994,     11,    714,   2474,    807,   1101,   4588,\n",
      "            330,   4340,    525,    498,   7521,    358,   1265,   2506,    432,\n",
      "           4586,    382,  14190,     11,    374,   1052,   4113,    770,    358,\n",
      "           1265,   2908,     30,  10696,    912,    264,   1795,   5239,   3405,\n",
      "            311,  14907,   4623,  16230,     11,    714,    279,   1196,   3207,\n",
      "            944,   2548,   4113,    770,     13,   2055,     11,  10282,    432,\n",
      "          63594,    323,   6785,   1265,    387,   1661,     13,   6771,    752,\n",
      "           2182,    429,    678,   3786,    624, 151668,    271,  13048,   1052,\n",
      "              0,    358,   2776,   6247,    311,   1492,      0,    358,   2776,\n",
      "           3730,   1632,    323,    358,   2776,   1588,    311,   7789,    498,\n",
      "            448,   4113,    498,   1184,      0,  26525,    232, 151645, 151643,\n",
      "         151643, 151643, 151643],\n",
      "        [151667,    198,  32313,     11,    279,   1196,    374,  10161,     11,\n",
      "            330,   4340,    525,    498,     11,   3351,   7521,   2055,    358,\n",
      "           1184,    311,   5889,    304,    264,  11657,    323,  10950,  11566,\n",
      "             13,   6771,    752,   1191,    553,  60608,    862,   3405,     13,\n",
      "            358,   1265,   1977,   2494,   1075,     11,    330,  18665,   1052,\n",
      "              0,   2585,    525,    498,   3351,   7521,   2938,    594,  30339,\n",
      "            323,   4933,    358,   2776,  14289,    382,   5847,     11,    358,\n",
      "           1366,    311,   3010,  12994,     13,  10696,   2548,    421,   1052,\n",
      "            594,   4113,    358,    646,   1492,    448,     13,   2938,    594,\n",
      "           1661,    311,   2506,    279,  10435,   1787,     13,    358,   1265,\n",
      "           1281,   2704,    847,   2033,    374,   6785,    323,   1787,  83075,\n",
      "             11,    773,    807,   2666,  10655,  11560,    803,    382,     40,\n",
      "           1265,   5648,    894,   8225,   4128,    323,   2506,    432,    678,\n",
      "           3100,     13,  10696,    912,    264,  15289,     88,   3579,    476,\n",
      "            264,  11657,  42365,    311,   2506,    279,  16232,   8205,     13,\n",
      "           7281,     11,   1779,    369,    894,  12752,  83789,    311,   5978,\n",
      "            279,   2033,    374,   8311,     13,   8704,    358,   2776,    458,\n",
      "          15235,     11,    358,   1513,    944,    614,   4345,  11449,     11,\n",
      "            773,    358,   1184,    311,   2506,    432,   4586,    382,  10061,\n",
      "            752,   2182,    432,    678,   3786,     25,    330,  18665,   1052,\n",
      "              0,   2585,    525,    498,   3351,     30,    358,   2776,   1588,\n",
      "            311,   1492,      0,  26525,    232,      1,   2938,   1265,   3421,\n",
      "            279,  58786,     13,   7093,  15934,    369,    894,  13580,    966,\n",
      "            476,  68125,  61072,   5975,     13,  21607,     11,    429,   4977,\n",
      "           1661,    624, 151668,    271,  18665,   1052,      0,   2585,    525,\n",
      "            498,   3351,     30,    358,   2776,   1588,    311,   1492,      0,\n",
      "          26525,    232, 151645]], device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer([text1, text2], padding=True, padding_side=\"right\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    output_hidden_states=True,\n",
    ") \n",
    "print(model_inputs)\n",
    "print(output.sequences[:,model_inputs.input_ids.shape[1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e965fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, the user asked, \"How are you?\" I need to respond in a friendly and helpful way. Let me think about the best approach.\n",
      "\n",
      "First, I should acknowledge their question. It's a simple greeting, so maybe start with \"Hi there!\" to show I'm happy to help. Then, I can add a bit more about my current state, like \"I'm doing well and I'm here to help you with anything!\" That gives a sense of being there for them.\n",
      "\n",
      "I should make sure the response is natural and not too formal. Maybe add a friendly emoji to keep it light. Also, check if the user has any specific questions or needs assistance, but since they just asked \"How are you?\" I should keep it general.\n",
      "\n",
      "Wait, is there anything else I should consider? Maybe add a follow-up question to encourage further interaction, but the user didn't ask anything else. So, keeping it concise and positive should be good. Let me put that all together.\n",
      "</think>\n",
      "content: Hi there! I'm happy to help! I'm doing well and I'm here to assist you with anything you need! 😊\n",
      "thinking content: <think>\n",
      "Okay, the user is asking, \"How are you, today?\" So I need to respond in a friendly and helpful manner. Let me start by acknowledging their question. I should say something like, \"Hey there! How are you today?\" That's straightforward and shows I'm listening.\n",
      "\n",
      "Next, I want to offer assistance. Maybe ask if there's anything I can help with. That's good to keep the conversation open. I should make sure my response is positive and open-ended, so they feel comfortable sharing more.\n",
      "\n",
      "I should avoid any negative language and keep it all light. Maybe add a smiley face or a friendly emoji to keep the tone warm. Also, check for any cultural nuances to ensure the response is appropriate. Since I'm an AI, I don't have personal experiences, so I need to keep it general.\n",
      "\n",
      "Let me put it all together: \"Hey there! How are you today? I'm here to help! 😊\" That should cover the essentials. Double-check for any typos or grammatical errors. Yeah, that seems good.\n",
      "</think>\n",
      "content: Hey there! How are you today? I'm here to help! 😊\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    output_ids = output.sequences[i][len(model_inputs.input_ids[i]):].tolist() \n",
    "    \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "933b2ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([47.2917,    -inf,    -inf,    -inf,    -inf], device='cuda:7'),\n",
      "indices=tensor([198,   2,   0,   3,   1], device='cuda:7'))\n",
      "torch.return_types.topk(\n",
      "values=tensor([52.7083,    -inf,    -inf,    -inf,    -inf], device='cuda:7'),\n",
      "indices=tensor([151667,      2,      0,      3,      1], device='cuda:7'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.topk(output.scores[0][0],5))\n",
    "print(torch.topk(output.scores[0][1],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f148854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aixue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
